{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and save DataFrames\n",
    "\n",
    "We do not cover all features of the packages. Please refer to their documentation to learn them.\n",
    "\n",
    "Here we'll load `CSV.jl` to read and write CSV files and `Arrow.jl`, `JLSO.jl`, and serialization, which allow us to work with a binary format and `JSONTables.jl` for JSON interaction. Finally we consider a custom `JDF.jl` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames\n",
    "using Arrow\n",
    "using CSV\n",
    "using Serialization\n",
    "using JLSO\n",
    "using JSONTables\n",
    "using CodecZlib\n",
    "using ZipFile\n",
    "using JDF\n",
    "using StatsPlots # for charts\n",
    "using Mmap # for compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a simple `DataFrame` for testing purposes,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = DataFrame(A=[true, false, true], B=[1, 2, missing],\n",
    "              C=[missing, \"b\", \"c\"], D=['a', missing, 'c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and use `eltypes` to look at the columnwise types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eltype.(eachcol(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `CSV` to save `x` to disk; make sure `x1.csv` does not conflict with some file in your working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV.write(\"x1.csv\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see how it was saved by reading `x.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(read(\"x1.csv\", String))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also load it back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = CSV.read(\"x1.csv\", DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when loading in a `DataFrame` from a `CSV` the column type for columns `:C` `:D` have changed to use special strings defined in the InlineStrings.jl package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eltype.(eachcol(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialization by JDF.jl and JLSO.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use serialization to save `x`.\n",
    "\n",
    "There are two ways to perform serialization. The first way is to use the `Serialization.serialize` as below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in general, this process will not work if the reading and writing are done by different versions of Julia, or an instance of Julia with a different system image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(\"x.bin\", \"w\") do io\n",
    "    serialize(io, x)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load back the saved file to `y` variable. Again `y` is identical to `x`. However, please beware that if you session does not have DataFrames.jl loaded, then it may not recognise the content as DataFrames.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = open(deserialize, \"x.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eltype.(eachcol(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JDF.jl\n",
    "\n",
    "[JDF.jl](https://github.com/xiaodaigh/JDF) is a relatively new package designed to serialize DataFrames. You can save a DataFrame with the `savejdf` function.\n",
    "\n",
    "For more details about design assumptions and limitations of `JDF.jl` please check out https://github.com/xiaodaigh/JDF.jl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JDF.save(\"x.jdf\", x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the saved JDF file, one can use the `loadjdf` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_loaded = JDF.load(\"x.jdf\") |> DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that they are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isequal(x_loaded, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JDF.jl offers the ability to load only certain columns from disk to help with working with large files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a JDFFile which is a on disk representation of `x` backed by JDF.jl\n",
    "x_ondisk = jdf\"x.jdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see all the names of `x` without loading it into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names(x_ondisk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below is an example of how to load only columns `:A` and `:D` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xd = JDF.load(x_ondisk; cols = [\"A\", \"D\"]) |> DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JLSO.jl\n",
    "\n",
    "Another way to perform serialization is by using the [JLSO.jl](https://github.com/invenia/JLSO.jl) library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JLSO.save(\"x.jlso\", :data => x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can laod back the file to `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = JLSO.load(\"x.jlso\")[:data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eltype.(eachcol(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSONTables.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often you might need to read and write data stored in JSON format. JSONTables.jl provides a way to process them in row-oriented or column-oriented layout. We present both options below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(io -> arraytable(io, x), \"x1.json\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(io -> objecttable(io, x), \"x2.json\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(read(\"x1.json\", String))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(read(\"x2.json\", String))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = open(jsontable, \"x1.json\") |> DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eltype.(eachcol(y1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = open(jsontable, \"x2.json\") |> DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eltype.(eachcol(y2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrow.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we use Apache Arrow format that allows, in particular, for data interchange with R or Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Arrow.write(\"x.arrow\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Arrow.Table(\"x.arrow\") |> DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eltype.(eachcol(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that columns of `y` are immutable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.A[1] = false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because `Arrow.Table` uses memory mapping and thus uses a custom vector types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get standard Julia Base vectors by copying a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = copy(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2.B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic bechmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create some files, so be careful that you don't already have these files in your working directory!\n",
    "\n",
    "In particular, we'll time how long it takes us to write a `DataFrame` with 1000 rows and 100000 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigdf = DataFrame(rand(Bool, 10^4, 1000), :auto)\n",
    "bigdf[!, 1] = Int.(bigdf[!, 1])\n",
    "bigdf[!, 2] = bigdf[!, 2] .+ 0.5\n",
    "bigdf[!, 3] = string.(bigdf[!, 3], \", as string\")\n",
    "println(\"First run\")\n",
    "println(\"CSV.jl\")\n",
    "csvwrite1 = @elapsed @time CSV.write(\"bigdf1.csv\", bigdf)\n",
    "println(\"Serialization\")\n",
    "serializewrite1 = @elapsed @time open(io -> serialize(io, bigdf), \"bigdf.bin\", \"w\")\n",
    "println(\"JDF.jl\")\n",
    "jdfwrite1 = @elapsed @time JDF.save(\"bigdf.jdf\", bigdf)\n",
    "println(\"JLSO.jl\")\n",
    "jlsowrite1 = @elapsed @time JLSO.save(\"bigdf.jlso\", :data => bigdf)\n",
    "println(\"Arrow.jl\")\n",
    "arrowwrite1 = @elapsed @time Arrow.write(\"bigdf.arrow\", bigdf)\n",
    "println(\"JSONTables.jl arraytable\")\n",
    "jsontablesawrite1 = @elapsed @time open(io -> arraytable(io, bigdf), \"bigdf1.json\", \"w\")\n",
    "println(\"JSONTables.jl objecttable\")\n",
    "jsontablesowrite1 = @elapsed @time open(io -> objecttable(io, bigdf), \"bigdf2.json\", \"w\")\n",
    "println(\"Second run\")\n",
    "println(\"CSV.jl\")\n",
    "csvwrite2 = @elapsed @time CSV.write(\"bigdf1.csv\", bigdf)\n",
    "println(\"Serialization\")\n",
    "serializewrite2 = @elapsed @time open(io -> serialize(io, bigdf), \"bigdf.bin\", \"w\")\n",
    "println(\"JDF.jl\")\n",
    "jdfwrite2 = @elapsed @time JDF.save(\"bigdf.jdf\", bigdf)\n",
    "println(\"JLSO.jl\")\n",
    "jlsowrite2 = @elapsed @time JLSO.save(\"bigdf.jlso\", :data => bigdf)\n",
    "println(\"Arrow.jl\")\n",
    "arrowwrite2 = @elapsed @time Arrow.write(\"bigdf.arrow\", bigdf)\n",
    "println(\"JSONTables.jl arraytable\")\n",
    "jsontablesawrite2 = @elapsed @time open(io -> arraytable(io, bigdf), \"bigdf1.json\", \"w\")\n",
    "println(\"JSONTables.jl objecttable\")\n",
    "jsontablesowrite2 = @elapsed @time open(io -> objecttable(io, bigdf), \"bigdf2.json\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedbar(\n",
    "    # Exclude JSONTables.jl arraytable due to timing\n",
    "    repeat([\"CSV.jl\", \"Serialization\", \"JDF.jl\", \"JLSO.jl\", \"Arrow.jl\", \"JSONTables.jl\\nobjecttable\"],\n",
    "            inner = 2),\n",
    "    [csvwrite1, csvwrite2, serializewrite1, serializewrite1, jdfwrite1, jdfwrite2,\n",
    "     jlsowrite1, jlsowrite2, arrowwrite1, arrowwrite2, jsontablesowrite2, jsontablesowrite2],\n",
    "    group = repeat([\"1st\", \"2nd\"], outer = 6),\n",
    "    ylab = \"Second\",\n",
    "    title = \"Write Performance\\nDataFrame: bigdf\\nSize: $(size(bigdf))\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = [\"bigdf1.csv\", \"bigdf.bin\", \"bigdf.arrow\", \"bigdf1.json\", \"bigdf2.json\"]\n",
    "df = DataFrame(file = data_files, size = getfield.(stat.(data_files), :size))\n",
    "append!(df, DataFrame(file = \"bigdf.jdf\", size=reduce((x,y)->x+y.size,\n",
    "                                                      stat.(joinpath.(\"bigdf.jdf\", readdir(\"bigdf.jdf\"))),\n",
    "                                                      init=0)))\n",
    "sort!(df, :size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@df df plot(:file, :size/1024^2, seriestype=:bar, title = \"Format File Size (MB)\", label=\"Size\", ylab=\"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"First run\")\n",
    "println(\"CSV.jl\")\n",
    "csvread1 = @elapsed @time CSV.read(\"bigdf1.csv\", DataFrame)\n",
    "println(\"Serialization\")\n",
    "serializeread1 = @elapsed @time open(deserialize, \"bigdf.bin\")\n",
    "println(\"JDF.jl\")\n",
    "jdfread1 = @elapsed @time JDF.load(\"bigdf.jdf\") |> DataFrame\n",
    "println(\"JLSO.jl\")\n",
    "jlsoread1 = @elapsed @time JLSO.load(\"bigdf.jlso\")\n",
    "println(\"Arrow.jl\")\n",
    "arrowread1 = @elapsed @time df_tmp = Arrow.Table(\"bigdf.arrow\") |> DataFrame\n",
    "arrowread1copy = @elapsed @time copy(df_tmp)\n",
    "println(\"JSONTables.jl arraytable\")\n",
    "jsontablesaread1 = @elapsed @time open(jsontable, \"bigdf1.json\")\n",
    "println(\"JSONTables.jl objecttable\")\n",
    "jsontablesoread1 = @elapsed @time open(jsontable, \"bigdf2.json\")\n",
    "println(\"Second run\")\n",
    "csvread2 = @elapsed @time CSV.read(\"bigdf1.csv\", DataFrame)\n",
    "println(\"Serialization\")\n",
    "serializeread2 = @elapsed @time open(deserialize, \"bigdf.bin\")\n",
    "println(\"JDF.jl\")\n",
    "jdfread2 = @elapsed @time JDF.load(\"bigdf.jdf\") |> DataFrame\n",
    "println(\"JLSO.jl\")\n",
    "jlsoread2 = @elapsed @time JLSO.load(\"bigdf.jlso\")\n",
    "println(\"Arrow.jl\")\n",
    "arrowread2 = @elapsed @time df_tmp = Arrow.Table(\"bigdf.arrow\") |> DataFrame\n",
    "arrowread2copy = @elapsed @time copy(df_tmp)\n",
    "println(\"JSONTables.jl arraytable\")\n",
    "jsontablesaread2 = @elapsed @time open(jsontable, \"bigdf1.json\")\n",
    "println(\"JSONTables.jl objecttable\")\n",
    "jsontablesoread2 = @elapsed @time open(jsontable, \"bigdf2.json\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude JSON\\narraytable arraytable due to much longer timing\n",
    "groupedbar(\n",
    "    repeat([\"CSV.jl\", \"Serialization\", \"JDF.jl\", \"JLSO.jl\", \"Arrow.jl\", \"Arrow.jl\\ncopy\", #\"JSON\\narraytable\",\n",
    "            \"JSON\\nobjecttable\"], inner = 2),\n",
    "    [csvread1, csvread2, serializeread1, serializeread2, jdfread1, jdfread2, jlsoread1, jlsoread2,\n",
    "     arrowread1, arrowread2, arrowread1+arrowread1copy, arrowread2+arrowread2copy,\n",
    "     # jsontablesaread1, jsontablesaread2,\n",
    "     jsontablesoread1, jsontablesoread2],    \n",
    "    group = repeat([\"1st\", \"2nd\"], outer = 7),\n",
    "    ylab = \"Second\",\n",
    "    title = \"Read Performance\\nDataFrame: bigdf\\nSize: $(size(bigdf))\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using gzip compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common user requirement is to be able to load and save CSV that are compressed using gzip. Below we show how this can be accomplished using `CodecZlib.jl`. The same pattern is applicable to `JSONTables.jl` compression/decompression.\n",
    "\n",
    "Again make sure that you do not have file named `df_compress_test.csv.gz` in your working directory.\n",
    "\n",
    "We first generate a random data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame(rand(1:10, 10, 1000), :auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GzipCompressorStream comes from CodecZlib\n",
    "\n",
    "open(\"df_compress_test.csv.gz\", \"w\") do io\n",
    "    stream = GzipCompressorStream(io)\n",
    "    CSV.write(stream, df)\n",
    "    close(stream)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = CSV.File(transcode(GzipDecompressor, Mmap.mmap(\"df_compress_test.csv.gz\"))) |> DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df == df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using zip files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you may have files compressed inside a zip file.\n",
    "\n",
    "In such a situation you may use [ZipFile.jl](https://github.com/fhs/ZipFile.jl) in conjunction an an appropriate reader to read the files.\n",
    "\n",
    "Here we first create a ZIP file and then read back its contents into a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = DataFrame(rand(1:10, 3, 4), :auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = DataFrame(rand(1:10, 3, 4), :auto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we show yet another way to write a `DataFrame` into a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a CSV file into the zip file\n",
    "w = ZipFile.Writer(\"x.zip\")\n",
    "\n",
    "f1 = ZipFile.addfile(w, \"x1.csv\")\n",
    "write(f1, sprint(show, \"text/csv\", df1))\n",
    "\n",
    "# write a second CSV file into zip file\n",
    "f2 = ZipFile.addfile(w, \"x2.csv\", method=ZipFile.Deflate)\n",
    "write(f2, sprint(show, \"text/csv\", df2))\n",
    "\n",
    "close(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we read the CSV we have written:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = ZipFile.Reader(\"x.zip\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the index index of file called x1.csv\n",
    "index_xcsv = findfirst(x->x.name == \"x1.csv\", z.files)\n",
    "# to read the x1.csv file in the zip file\n",
    "df1_2 = CSV.read(read(z.files[index_xcsv]), DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_2 == df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the index index of file called x2.csv\n",
    "index_xcsv = findfirst(x->x.name == \"x2.csv\", z.files)\n",
    "# to read the x2.csv file in the zip file\n",
    "df2_2 = CSV.read(read(z.files[index_xcsv]), DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_2 == df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that once you read a given file from `z` object its stream is all used-up (it is at its end). Therefore to read it again you need to close `z` and open it again.\n",
    "\n",
    "Also do not forget to close the zip file once done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's clean up. Do not run the next cell unless you are sure that it will not erase your important files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foreach(rm, [\"x1.csv\", \"x.bin\", \"x.jlso\", \"x1.json\", \"x2.json\",\n",
    "             \"bigdf1.csv\", \"bigdf.bin\", \"bigdf.jlso\", \"bigdf1.json\", \"bigdf2.json\", \n",
    "             \"x.zip\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm(\"bigdf.jdf\", recursive=true)\n",
    "rm(\"x.jdf\", recursive=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we did not remove `x.arrow` and `bigdf.arrow` and `df_compress_test.csv.gz` - you have to do it manually, as these files are memory mapped."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
