{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "153e6d34",
   "metadata": {},
   "source": [
    "# Load and save DataFrames\n",
    "\n",
    "We do not cover all features of the packages. Please refer to their documentation to learn them.\n",
    "\n",
    "- https://github.com/apache/arrow-julia (Arrow.jl)\n",
    "- https://github.com/JuliaData/JSONTables.jl\n",
    "- https://github.com/JuliaIO/JLD2.jl\n",
    "\n",
    "Here we'll load `CSV.jl` to read and write CSV files and `Arrow.jl` and `JLD2.jl`, which allow us to work with a binary format, and finally `JSONTables.jl` for JSON interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d82903",
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames\n",
    "using Arrow\n",
    "using CSV\n",
    "using JSONTables\n",
    "using CodecZlib\n",
    "using ZipFile\n",
    "using JLD2\n",
    "using StatsPlots ## for charts\n",
    "using Mmap ## for compression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06a5e35",
   "metadata": {},
   "source": [
    "Let's create a simple `DataFrame` for testing purposes,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7c35b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = DataFrame(\n",
    "    A=[true, false, true], B=[1, 2, missing],\n",
    "    C=[missing, \"b\", \"c\"], D=['a', missing, 'c']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9276e377",
   "metadata": {},
   "source": [
    "and use `eltypes` to look at the column-wise types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cad0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eltype.(eachcol(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bfe825",
   "metadata": {},
   "source": [
    "## CSV.jl\n",
    "\n",
    "Let's use `CSV.jl` to save `x` to disk; make sure `x1.csv` does not conflict with some file in your working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a40991",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpdir = mktempdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545ff118",
   "metadata": {},
   "outputs": [],
   "source": [
    "location = joinpath(tmpdir, \"x1.csv\")\n",
    "CSV.write(location, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb024ac",
   "metadata": {},
   "source": [
    "Now we can see how it was saved by reading `x.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe846d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(read(location, String))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0042b43",
   "metadata": {},
   "source": [
    "We can also load it back as a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c996ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = CSV.read(location, DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd04b3d",
   "metadata": {},
   "source": [
    "Note that when loading in a `DataFrame` from a `CSV` the column type for columns `:C` `:D` have changed to use special strings defined in the InlineStrings.jl package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d638c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "eltype.(eachcol(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2433afc",
   "metadata": {},
   "source": [
    "## JSONTables.jl\n",
    "\n",
    "Often you might need to read and write data stored in JSON format. `JSONTables.jl` provides a way to process them in row-oriented or column-oriented layout. We present both options below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa11e21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "location1 = joinpath(tmpdir, \"x1.json\")\n",
    "open(io -> arraytable(io, x), location1, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343fa151",
   "metadata": {},
   "outputs": [],
   "source": [
    "location2 = joinpath(tmpdir, \"x2.json\")\n",
    "open(io -> objecttable(io, x), location2, \"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6142e012",
   "metadata": {},
   "source": [
    "Read them back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b819f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(read(location1, String))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601b5d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(read(location2, String))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a87234",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = open(jsontable, location1) |> DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398db41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eltype.(eachcol(y1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3234e8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = open(jsontable, location2) |> DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbda567",
   "metadata": {},
   "outputs": [],
   "source": [
    "eltype.(eachcol(y2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adff04fd",
   "metadata": {},
   "source": [
    "## JLD2.jl\n",
    "\n",
    "`JLD2.jl` is a high-performance, pure Julia library for saving and loading arbitrary Julia data structures, with HDF5 format.\n",
    "\n",
    "Documentation: https://juliaio.github.io/JLD2.jl/dev/basic_usage/\n",
    "\n",
    "- `save()` and `load()`: General save and load using the `FileIO.jl` interface\n",
    "- `jldsave()` and `jldloac()`: Advanced save and load with more options\n",
    "- `save_object()` and `load_object()`: Single-object load and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49b8269",
   "metadata": {},
   "outputs": [],
   "source": [
    "using JLD2\n",
    "location = joinpath(tmpdir, \"x.jld2\")\n",
    "\n",
    "save_object(location, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c89b67",
   "metadata": {},
   "source": [
    "Read it back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a756f1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_object(location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7849b4",
   "metadata": {},
   "source": [
    "## Arrow.jl\n",
    "\n",
    "Finally we use Apache Arrow format that allows, in particular, for data interchange with R or Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f387c5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "location = joinpath(tmpdir, \"x.arrow\")\n",
    "Arrow.write(location, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc0a0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Arrow.Table(location) |> DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dc0ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eltype.(eachcol(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa9c761",
   "metadata": {},
   "source": [
    "Note that columns of `y` are immutable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ef11f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try\n",
    "    y.A[1] = false\n",
    "catch e\n",
    "    show(e)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d991f951",
   "metadata": {},
   "source": [
    "This is because `Arrow.Table` uses memory mapping and thus uses a custom vector types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92db145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf491422",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2200409e",
   "metadata": {},
   "source": [
    "You can get standard Julia Base vectors by copying a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1167f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = copy(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dca611",
   "metadata": {},
   "outputs": [],
   "source": [
    "y2.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59649fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y2.B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd30e638",
   "metadata": {},
   "source": [
    "## Basic benchmarking\n",
    "\n",
    "Next, we'll create some files in the temp directory.\n",
    "\n",
    "In particular, we'll time how long it takes us to write a `DataFrame` with 1000 rows and 100000 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a672fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigdf = DataFrame(rand(Bool, 10^4, 1000), :auto)\n",
    "\n",
    "bigdf[!, 1] = Int.(bigdf[!, 1])\n",
    "bigdf[!, 2] = bigdf[!, 2] .+ 0.5\n",
    "bigdf[!, 3] = string.(bigdf[!, 3], \", as string\")\n",
    "\n",
    "tmpdir = mktempdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45b1d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"First run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ed7c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"CSV.jl\")\n",
    "fname = joinpath(tmpdir, \"bigdf1.csv.gz\")\n",
    "csvwrite1 = @elapsed @time CSV.write(fname, bigdf; compress=true)\n",
    "\n",
    "println(\"Arrow.jl\")\n",
    "fname = joinpath(tmpdir, \"bigdf.arrow\")\n",
    "arrowwrite1 = @elapsed @time Arrow.write(fname, bigdf)\n",
    "\n",
    "println(\"JSONTables.jl arraytable\")\n",
    "fname = joinpath(tmpdir, \"bigdf1.json\")\n",
    "jsontablesawrite1 = @elapsed @time open(io -> arraytable(io, bigdf), fname, \"w\")\n",
    "\n",
    "println(\"JSONTables.jl objecttable\")\n",
    "fname = joinpath(tmpdir, \"bigdf2.json\")\n",
    "jsontablesowrite1 = @elapsed @time open(io -> objecttable(io, bigdf), fname, \"w\")\n",
    "\n",
    "println(\"JLD2.jl\")\n",
    "fname = joinpath(tmpdir, \"bigdf.jld2\")\n",
    "jld2write1 = @elapsed @time save_object(fname, bigdf; compress = ZstdFilter())\n",
    "\n",
    "println(\"Second run\")\n",
    "\n",
    "println(\"CSV.jl\")\n",
    "fname = joinpath(tmpdir, \"bigdf1.csv.gz\")\n",
    "csvwrite2 = @elapsed @time CSV.write(fname, bigdf; compress=true)\n",
    "\n",
    "println(\"Arrow.jl\")\n",
    "fname = joinpath(tmpdir, \"bigdf.arrow\")\n",
    "arrowwrite2 = @elapsed @time Arrow.write(fname, bigdf)\n",
    "\n",
    "println(\"JSONTables.jl arraytable\")\n",
    "fname = joinpath(tmpdir, \"bigdf1.json\")\n",
    "jsontablesawrite2 = @elapsed @time open(io -> arraytable(io, bigdf), fname, \"w\")\n",
    "\n",
    "println(\"JSONTables.jl objecttable\")\n",
    "fname = joinpath(tmpdir, \"bigdf2.json\")\n",
    "jsontablesowrite2 = @elapsed @time open(io -> objecttable(io, bigdf), fname, \"w\")\n",
    "\n",
    "println(\"JLD2.jl\")\n",
    "fname = joinpath(tmpdir, \"bigdf.jld2\")\n",
    "jld2write2 = @elapsed @time save_object(fname, bigdf; compress = ZstdFilter());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288ce519",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedbar(\n",
    "    repeat([\"CSV.jl (gz)\", \"Arrow.jl\", \"JSONTables.jl\\nobjecttable\", \"JLD2.jl\"],\n",
    "        inner=2),\n",
    "    [csvwrite1, csvwrite2, arrowwrite1, arrowwrite2, jsontablesowrite1, jsontablesowrite2, jld2write1, jld2write2],\n",
    "    group=repeat([\"1st\", \"2nd\"], outer=4),\n",
    "    ylab=\"Second\",\n",
    "    title=\"Write Performance\\nDataFrame: bigdf\\nSize: $(size(bigdf))\",\n",
    "    permute = (:x, :y)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc78f20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = [\"bigdf1.csv.gz\", \"bigdf.arrow\", \"bigdf1.json\", \"bigdf2.json\", \"bigdf.jld2\"] .|> (f -> joinpath(tmpdir, f))\n",
    "df = DataFrame(file=[\"CSV.jl (gz)\", \"Arrow.jl\", \"objecttable\", \"arraytable\", \"JLD2.jl\"], size=getfield.(stat.(data_files), :size))\n",
    "sort!(df, :size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e409b32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@df df plot(:file, :size / 1024^2, seriestype=:bar, title=\"Format File Size (MB)\", label=\"Size\", ylab=\"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33199bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"First run\")\n",
    "\n",
    "println(\"CSV.jl\")\n",
    "fname = joinpath(tmpdir, \"bigdf1.csv.gz\")\n",
    "csvread1 = @elapsed @time CSV.read(fname, DataFrame)\n",
    "\n",
    "println(\"Arrow.jl\")\n",
    "fname = joinpath(tmpdir, \"bigdf.arrow\")\n",
    "arrowread1 = @elapsed @time df_tmp = Arrow.Table(fname) |> DataFrame\n",
    "arrowread1copy = @elapsed @time copy(df_tmp)\n",
    "\n",
    "println(\"JSONTables.jl arraytable\")\n",
    "fname = joinpath(tmpdir, \"bigdf1.json\")\n",
    "jsontablesaread1 = @elapsed @time open(jsontable, fname)\n",
    "\n",
    "println(\"JSONTables.jl objecttable\")\n",
    "fname = joinpath(tmpdir, \"bigdf2.json\")\n",
    "jsontablesoread1 = @elapsed @time open(jsontable, fname)\n",
    "\n",
    "println(\"JLD2.jl\")\n",
    "fname = joinpath(tmpdir, \"bigdf.jld2\")\n",
    "jld2read1 = @elapsed @time load_object(fname)\n",
    "\n",
    "println(\"Second run\")\n",
    "fname = joinpath(tmpdir, \"bigdf1.csv.gz\")\n",
    "csvread2 = @elapsed @time CSV.read(fname, DataFrame)\n",
    "\n",
    "println(\"Arrow.jl\")\n",
    "fname = joinpath(tmpdir, \"bigdf.arrow\")\n",
    "arrowread2 = @elapsed @time df_tmp = Arrow.Table(fname) |> DataFrame\n",
    "arrowread2copy = @elapsed @time copy(df_tmp)\n",
    "\n",
    "println(\"JSONTables.jl arraytable\")\n",
    "fname = joinpath(tmpdir, \"bigdf1.json\")\n",
    "jsontablesaread2 = @elapsed @time open(jsontable, fname)\n",
    "\n",
    "println(\"JSONTables.jl objecttable\")\n",
    "fname = joinpath(tmpdir, \"bigdf2.json\")\n",
    "jsontablesoread2 = @elapsed @time open(jsontable, fname)\n",
    "\n",
    "println(\"JLD2.jl\")\n",
    "fname = joinpath(tmpdir, \"bigdf.jld2\")\n",
    "jld2read2 = @elapsed @time load_object(fname);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e49238f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude JSONTables due to much longer timing\n",
    "groupedbar(\n",
    "    repeat([\"CSV.jl (gz)\", \"Arrow.jl\", \"Arrow.jl\\ncopy\", ##\"JSON\\narraytable\",\n",
    "            \"JSON\\nobjecttable\", \"JLD2.jl\"], inner=2),\n",
    "    [csvread1, csvread2, arrowread1, arrowread2, arrowread1 + arrowread1copy, arrowread2 + arrowread2copy,\n",
    "        ## jsontablesaread1, jsontablesaread2,\n",
    "        jsontablesoread1, jsontablesoread2, jld2read1, jld2read2],\n",
    "    group=repeat([\"1st\", \"2nd\"], outer=5),\n",
    "    ylab=\"Second\",\n",
    "    title=\"Read Performance\\nDataFrame: bigdf\\nSize: $(size(bigdf))\",\n",
    "    permute = (:x, :y)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a86c25",
   "metadata": {},
   "source": [
    "## Using gzip compression\n",
    "A common user requirement is to be able to load and save CSV that are compressed using gzip. Below we show how this can be accomplished using `CodecZlib.jl`.\n",
    "\n",
    "Again make sure that you do not have file named `df_compress_test.csv.gz` in your working directory.\n",
    "We first generate a random data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20894be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame(rand(1:10, 10, 1000), :auto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec0e2ca",
   "metadata": {},
   "source": [
    "Use `compress=true` option to compress the CSV with the `gz` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24248d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpdir = mktempdir()\n",
    "fname = joinpath(tmpdir, \"df_compress_test.csv.gz\")\n",
    "CSV.write(fname, df; compress=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b984b2e1",
   "metadata": {},
   "source": [
    "Read the CSV file back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8d7aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = CSV.File(fname) |> DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab866104",
   "metadata": {},
   "outputs": [],
   "source": [
    "df == df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535b1079",
   "metadata": {},
   "source": [
    "## Working with zip files\n",
    "\n",
    "Sometimes you may have files compressed inside a zip file.\n",
    "In such a situation you may use [ZipFile.jl](https://github.com/fhs/ZipFile.jl) in conjunction an an appropriate reader to read the files.\n",
    "Here we first create a ZIP file and then read back its contents into a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696c9b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = DataFrame(rand(1:10, 3, 4), :auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84e2485",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = DataFrame(rand(1:10, 3, 4), :auto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06614336",
   "metadata": {},
   "source": [
    "And we show yet another way to write a `DataFrame` into a CSV file:\n",
    "Writing a CSV file into the zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18984620",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = ZipFile.Writer(joinpath(tmpdir, \"x.zip\"))\n",
    "\n",
    "f1 = ZipFile.addfile(w, \"x1.csv\")\n",
    "write(f1, sprint(show, \"text/csv\", df1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9088948",
   "metadata": {},
   "source": [
    "write a second CSV file into the zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9b883b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = ZipFile.addfile(w, \"x2.csv\", method=ZipFile.Deflate)\n",
    "write(f2, sprint(show, \"text/csv\", df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09db0ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "close(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa18798f",
   "metadata": {},
   "source": [
    "Now we read the compressed CSV file we have written:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c7251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = ZipFile.Reader(joinpath(tmpdir, \"x.zip\"))\n",
    "## find the index index of file called x1.csv\n",
    "index_xcsv = findfirst(x -> x.name == \"x1.csv\", r.files)\n",
    "## to read the x1.csv file in the zip file\n",
    "df1_2 = CSV.read(read(r.files[index_xcsv]), DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ed407c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_2 == df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c8bc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## find the index index of file called x2.csv\n",
    "index_xcsv = findfirst(x -> x.name == \"x2.csv\", r.files)\n",
    "## to read the x2.csv file in the zip file\n",
    "df2_2 = CSV.read(read(r.files[index_xcsv]), DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8d92cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_2 == df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb5a672",
   "metadata": {},
   "source": [
    "Note that once you read a given file from `r` object its stream is all used-up (reaching its end). Therefore to read it again you need to close the file object `r` and open it again.\n",
    "Also do not forget to close the zip file once you are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e59ad8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "close(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.12.2",
   "language": "julia",
   "name": "julia-1.12"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
